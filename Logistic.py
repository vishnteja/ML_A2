# -*- coding: utf-8 -*-
"""ML-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wN5rpr-w-whvWKdYgnZ0_RCRunTM7roO

Pre-processing
"""

import csv
from numpy import *
import time
import copy
data = list()
with open('data_logistic.txt',newline = '') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        data.append(list(map(float, row)))
data = array(data)
label = data[:,4]
colors = ['red', 'blue']
data

"""Sigmoid function"""

def sigmoid(x):
  return 1 / (1 + exp(-x))

"""Logistic regression implememtation"""

class logistic:
  
  def __init__(self, dataset, learningRate, randomised = False ,train_test_split = 0.8):
    dataset_copy = copy.deepcopy(dataset)
    random.shuffle(dataset_copy)
    self.train = dataset_copy[:int(train_test_split*len(dataset)), : ]
    self.test = dataset_copy[int(train_test_split*len(dataset)):, :]
    self.alpha = learningRate
    self.theta = ones(shape(self.train)[1])
    if randomised==True:
      self.theta = random.rand(shape(self.train)[1])*2-1
  
  def accuracy(self):
    x1 = self.test[:, :-1]
    x = ones((len(x1), x1.shape[1] + 1))             # Finding error with test set
    x[:, 1:] = x1
    y = self.test[:, shape(self.test)[1]-1]
    hypothesis = sigmoid(dot(x, self.theta))
    diff = rint(hypothesis) - y
    return 1 - count_nonzero(diff)/(shape(self.test)[0])
  
  def gradientDescent(self, num_iterations, printCost=False, regularised = False):
    if(regularised):
      self.validation = copy.deepcopy(self.train[int(0.75*len(self.train)):, :])
      self.train = self.train[:int(0.75*len(self.train)), : ]
    
    column_mean = mean(self.train[:, :-1], axis=0)            # feature scaling
    column_std = std(self.train[:, :-1], axis=0)
    self.train[:, :-1] = (self.train[:, :-1] - column_mean) / column_std

    x1 = self.train[:, :-1]
    x = ones((len(x1), x1.shape[1] + 1))         # Adding the bias
    x[:, 1:] = x1
    y = self.train[:, shape(self.train)[1]-1]
    m, n = shape(x)

    xT = x.transpose()
    cost_prev = 0
    
    if(regularised == False):
      numit = 0
      for i in range(0, num_iterations):
        numit = i
        hypothesis = sigmoid(dot(x, self.theta))                   #Predicting value using coefficient vector
        temp_loss = hypothesis - y
        cost = -sum( y*log(hypothesis) + (1-y)*log(1-hypothesis) ) / (m)         #Calculate loss
        if(printCost and i%100 == 0): print("Iteration %d and Cost: %f" % (i, cost))
        gradient = dot(xT, temp_loss)  #Find gradient  (Imp steps)
        #print(shape(gradient))
        self.theta = self.theta - self.alpha * gradient                #Modify feature value (Imp steps)
        if abs(cost_prev - cost) < 1e-10:
          break
        cost_prev = cost
      print("Iterations: " + str(numit))
      self.theta[1:] /= column_std                             #Training done. Remove feature scaling
      self.theta[0] -= sum(self.theta[1:] * column_mean)
      
    else:
      max_acc = 0
      min_lamda = 0
      min_theta = self.theta
      theta_copy = copy.deepcopy(self.theta)
      lamda_cost = []
      for lamda in range(0,20,2):
        print("In lamda: " + str(lamda))
        self.theta = copy.deepcopy(theta_copy)
        for i in range(0, num_iterations):
          hypothesis = sigmoid(dot(x, self.theta))                   #Predicting value using coefficient vector
          temp_loss = hypothesis - y
          cost = -sum( y*log(hypothesis) + (1-y)*log(1-hypothesis) ) / (m)         #Calculate loss
          if(printCost and i%100 == 0): print("Iteration %d and Cost: %f" % (i, cost))
          gradient = dot(xT, temp_loss) + lamda*self.theta #Find gradient  (Imp steps)
          #print(shape(gradient))
          gradient[0] -= lamda*self.theta[0]
          self.theta = self.theta - self.alpha * gradient                #Modify feature value (Imp steps)
          if abs(cost_prev - cost) < 1e-10:
            break
          cost_prev = cost
        self.theta[1:] /= column_std                             #Training done. Remove feature scaling
        self.theta[0] -= sum(self.theta[1:] * column_mean)

        x2 = self.validation[:, :-1]
        x3 = ones((len(x2), x2.shape[1] + 1))             # Finding error with test set
        x3[:, 1:] = x2
        y3 = self.validation[:, shape(self.validation)[1]-1]
        hypothesis = sigmoid(dot(x3, self.theta))
        diff = rint(hypothesis) - y3
        temp_accuracy = 1-count_nonzero(diff)/(shape(self.validation)[0])
        lamda_cost.append(temp_accuracy)
        if(temp_accuracy > max_acc):
          min_lamda = lamda
          min_theta = copy.deepcopy(self.theta)
          max_acc = temp_accuracy
        self.getWeights()  
      
      self.theta = min_theta
      return lamda_cost
    
  def predict(self, x):
    x.insert(0, 1)
    return rint(sigmoid(dot(x, self.theta)))
  
  def getWeights(self):
    print("Coefficients vector: " + str(self.theta))
    #return self.theta

"""Testing the implementation"""

model = logistic(data, 0.075) 
#model.gradientDescent(100000, False, False)
lamda_cost = model.gradientDescent(100000, False, True)

print(model.accuracy())

#print(model.predict([0.39012,-0.14279,-0.031994,0.35084]))
#model.gradientDescentR(100000, False)

# import matplotlib.pyplot as plt
# x = [i for i in range(0,30,3)]
# plt.plot(x,lamda_cost)
# plt.xlabel("Lamda")
# plt.ylabel("Accuracy")
# plt.show()

# """Compare with sklearn"""

# from sklearn.datasets import load_iris
# from sklearn.linear_model import LogisticRegression
# x = data[:,:-1]
# y = data[:,shape(data)[1]-1:]
# clf = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr').fit(x, squeeze(y))
# print(clf.score(x,y))
# print(clf.coef_)
# print(clf.intercept_)

